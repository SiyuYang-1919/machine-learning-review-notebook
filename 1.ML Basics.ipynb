{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition: \n",
    "- ML algorithms build models based on sample data, know as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.\n",
    "\n",
    "## 2. Object:\n",
    "- Data (values of attributes):\n",
    "  - Numeric: normalization\n",
    "  - Categorical: mutually exclusive; encoded as numbers; synonymy problem; only $=$ and $!=$ are meaningful\n",
    "  - Ordinal: have order; encoded as numbers to preserve ordering; cannot add/multiply...only $<$, $>$, and $=$ are meaningful; hard to tell attributes have ordering or not\n",
    "\n",
    "## 3. Aim:\n",
    "- Consider which model should be learnt and how to learn models to achieve accurate prediction and analysis with highest possible efficiency.\n",
    "\n",
    "## 4. Process:\n",
    "<p align=\"center\">\n",
    "<img src=images/1-1.png width=\"400\" height=\"150\" alt=\"5\" align=centering>\n",
    "\n",
    "- 1. Get a limited training set;\n",
    "- 2. Confirm the hypothesis space (a space including all the possible models);\n",
    "- 3. Make the learining strategy or the standard to choose a model；\n",
    "- 4. Use algorithms to solve the optimal model；\n",
    "- 5. Choose the optimal model；\n",
    "- 6. Use the model to analyze or predict new data.\n",
    "\n",
    "## 5. Basic Categories: \n",
    "### 5.1 Supervised and Unsupervised model:\n",
    "#### Supervised\n",
    "- learning predictive models from labelled data.\n",
    "<p align=\"center\">\n",
    "<img src=images/1-2.png width=\"400\" height=\"250\" alt=\"5\" align=centering>\n",
    "\n",
    ", where $y_{N+1} = arg max_{y}\\hat{P}(y|x_{N+1})$ or $y_{N+1} = \\hat{f}(x_{N+1})$\n",
    "- More specifically, there are three problems in supervised learning:\n",
    "  - Classification\n",
    "  - Regression\n",
    "  - Tagging\n",
    "\n",
    "#### Unsupervised\n",
    "- learning predictive models from unlabelled data.\n",
    "<p align=\"center\">\n",
    "<img src=images/1-3.png width=\"400\" height=\"300\" alt=\"5\" align=centering>\n",
    "\n",
    "- Reinforcement\n",
    "- Semi-supervised learning\n",
    "- Active learning\n",
    "\n",
    "### 5.2 Probabilistic and deterministic model:\n",
    "- Main difference is in the inner structure: a probabilistic model can be expressed as joint probability distribution whereas a non-probabilistic model often cannnot. \n",
    "- Logistics model can be viewed as both\n",
    "\n",
    "#### Probabilistic model:\n",
    "- $P(y|x)$ or $P(z|x)$, $P(x|z)$\n",
    "- Decision trees; naive bayes; GMM\n",
    "\n",
    "#### Deterministic model:\n",
    "- $y = f(x)$\n",
    "- SVM, KNN, AdaBoost, K-means, neural networks\n",
    "\n",
    "#### Generative and discriminative approach(model)\n",
    "- Generative: learn the joint probabilistic distribution $P(X,Y)$, then compute P(Y|X) as the predictive model:\n",
    "$$ P(Y|X) = \\frac{P(X,Y)}{P(X)} $$ \n",
    "including naive bayes.\n",
    "- Discriminative: only get the predictions/decisions, including knn, dt, logistics regression, svm, boosting methods, etc.\n",
    "\n",
    "### 5.3 Parametric and non-parametric model:\n",
    "- Whether the dimension of a model is fixed and limited\n",
    "- P: Naive bayes, logistics regression, k-means, GMM\n",
    "- NP: DT, SVM, AdaBoost, KNN\n",
    "\n",
    "### 5.4 Bayesian learning and kernel method:\n",
    "\n",
    "#### Bayesian learning\n",
    "- Calculate the probability of a model given certain data or posterior probability. \n",
    "<p align=\"center\">\n",
    "<img src=images/1-4.png width=\"400\" height=\"150\" alt=\"5\" align=centering>\n",
    "\n",
    "#### Kernal method\n",
    "- Kernel SVM, PCA, and k-means\n",
    "<p align=\"center\">\n",
    "<img src=images/1-5.png width=\"400\" height=\"200\" alt=\"5\" align=centering>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Three elements of machine learning\n",
    "\n",
    "### 6.1 model\n",
    "- for supervised learning, model is the conditional probability distribution and decision function\n",
    "- hypothesis space for decision functions:\n",
    "$$ F = {f|Y=f_{\\theta}(X),\\theta \\in R^{n}} $$\n",
    "- hypothesis sapce for conditional probability distribution:\n",
    "$$ F = {P|P(Y|X),\\theta \\in R^{n}} $$\n",
    "\n",
    "### 6.2 strategy\n",
    "- according to what standards to learn or choose the optimal model\n",
    "\n",
    "#### Loss (cost) function and risk function (expected loss):\n",
    "- Loss(cost) function:\n",
    "  - 0-1 loss function\n",
    "  \n",
    "  $ L(Y,f(X)) = \n",
    "  \\begin{cases}\n",
    "  1, Y!=f(X)\\\\\n",
    "  0, Y=f(X)\n",
    "  \\end{cases}$\n",
    "\n",
    "  - quadratic loss function\n",
    "  $$  L(Y,f(X)) = (Y-f(X))^2 $$\n",
    "  - absolute loss function\n",
    "  $$ L(Y,f(X)) = |Y-f(X)| $$\n",
    "  - logarithmic loss function\n",
    "  $$ L(Y,P(Y|X)) = -log P(Y|X) $$\n",
    "\n",
    "- Risk function:\n",
    "$$ R_{exp}(f) = E_{p}[L(Y,f(X))] = \\int_{x\\text{x}y} L(y,f(x))P(x,y) \\,{\\rm d}x{\\rm d}y $$\n",
    "\n",
    "However, we do not know P(X,Y), but can only get empirical loss based on given training data. According to the law of large number, the empirical risk should be close to the expected risk when N is infinite. \n",
    "$$ R_{emp}(f) = \\frac{1}{N} \\sum_{i=1}^N L(y_{i},f(x_{i})) $$\n",
    "\n",
    "#### Minimization strategy\n",
    "- ERM(empirical risk minimization)\n",
    "  - The optimal model is the one with minimal empirical risk, the minimization function:\n",
    "  $$ \\text{min}_{f \\in F} \\frac{1}{N}\\sum_{i=1}^N L(y_{i},f(x_{i}))$$\n",
    "- SRM(structural risk minimization)\n",
    "  - To prevent over-fitting problem, a regularizatio item is added, the structural risk:\n",
    "  $$ R_{srm}(f) = \\frac{1}{N}\\sum_{i=1}^N L(y_{i},f(x_{i})) + \\lambda J(f) $$\n",
    "  The minimization function:\n",
    "  $$ \\text{min}_{f \\in F} \\frac{1}{N}\\sum_{i=1}^N L(y_{i},f(x_{i}))$$\n",
    "\n",
    "### 6.3 algorithm\n",
    "- How to solve the minimization function;\n",
    "- ML problem is an optimization problem;\n",
    "- The algorithm of ML is the algorithm to solve the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation and selection\n",
    "\n",
    "### Training error and test error\n",
    "- Training error reflects whether it is easy to learn some problem;\n",
    "- Test error reflects the generalization ability.\n",
    "\n",
    "### Overfitting problem\n",
    "- Include many parameters;\n",
    "- Predict well on the training data;\n",
    "- Bad prediction on the test data.\n",
    "<p align=\"center\">\n",
    "<img src=images/1-6.png width=\"400\" height=\"250\" alt=\"5\" align=centering>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Regularization and cross-validation\n",
    "\n",
    "### 8.1 Regularization\n",
    "- Regularization item is to balance the complexity of a model and its predictability.\n",
    "- L1 Regularization item:\n",
    "$$ L_(w) = \\frac{1}{N}\\sum_{i=1}^N (f(x_{i};w)-y_{i})^2 + \\lambda ||w|| $$\n",
    "\n",
    "- L2 Regularization item:\n",
    "$$ L_(w) = \\frac{1}{N}\\sum_{i=1}^N (f(x_{i};w)-y_{i})^2 + \\frac{1}{2}\\lambda ||w||^2 $$\n",
    "\n",
    "### 8.2 Cross-validation\n",
    "- training set is to train models (construct model); validation set is to choose models (pick model and knob setting); test set is to evaluate the final model (estimate the future error rate).\n",
    "- cross-validation is to choose the optimal model.\n",
    "- 1) simple: 70-30\n",
    "- 2) S-fold cross validation\n",
    "- 3) leave-one-out cross validation (S=N)\n",
    "\n",
    "### 8.3 Problems with leave-one-out cross validation\n",
    "- High computational cost;\n",
    "- Imbalanced classes\n",
    "- Stratification can help with this issue:\n",
    "  - randomly split each class into K parts at first\n",
    "  - assemble $i_{th}$ part from all classes to make the $i_{th}$ fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generalization ability\n",
    "### 9.1 Generalization error\n",
    "- The generalization error, equal to the expected loss, is measured by:\n",
    "$$ R_{exp}(\\hat{f}) = E_{p}[L(Y,\\hat{f}(X))] = \\int_{x\\text{x}y} L(y,\\hat{f}(x))P(x,y) \\,{\\rm d}x{\\rm d}y $$\n",
    "\n",
    "### 9.2 Generalization error bound\n",
    "- The generalization ability is measured by generalization error bound:\n",
    "  - Some properties: 1) the generalization error bound is close to 0 when R increases; 2) the generalization error bound is greater if the hypothesis space is larger.\n",
    "\n",
    "### 9.3 Using testing error as an estimate\n",
    "- As we don't know the probability distribution in the function above, we cannot calculate the true generalization error. However, we can use testing error to estimate. When the number of test samples is very large, the testing error will be very close to the true generalization error."
   ]
  },
  {
   "source": [
    "## 10. Evaluation\n",
    "\n",
    "### 10.1 Evaluating regression\n",
    "- (root) mean squared error;\n",
    "- mean(median) absolute error;\n",
    "- correlation coefficiant\n",
    "\n",
    "### 10.2 Evaluating classification\n",
    "<p align=\"center\">\n",
    "<img src=images/1-7.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- classification error:\n",
    "$$ \\frac{FP+FN}{TP+TN+FP+FN} $$\n",
    "- accuracy:\n",
    "$$ \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "However, these indicators cannot handle unbalanced classes, but we can use:\n",
    "- recall rate: % of postives in reality that we classified correctly\n",
    "$$ \\frac{TP}{TP+FN} $$\n",
    "- precision rate: % of what we predict as positives are actually postive\n",
    "$$ \\frac{TP}{TP+FP} $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/1-8.png width=\"400\" height=\"100\" alt=\"5\" align=centering>\n",
    "\n",
    "- ROC:\n",
    "<p align=\"center\">\n",
    "<img src=images/1-9.png width=\"400\" height=\"300\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}