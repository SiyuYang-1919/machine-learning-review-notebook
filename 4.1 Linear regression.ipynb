{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Linear model\n",
    "$$ f(x) = w_{0} + w_{1}x_{1} + ... + w_{D}x_{D} $$\n",
    "$$ = \\mathbf{W}^T\\mathbf{X} $$\n",
    "To generalize with more complex regression models, we rewrite:\n",
    "$$ f(x) = \\mathbf{W}^T\\phi{(\\mathbf{X})} $$\n",
    "\n",
    "- ```Note:```\n",
    "We can easily transform the original attributes $\\mathbf{x}$ non-linearly into $\\phi(\\mathbf{x})$ and still do the linear on them:\n",
    "<p align=\"center\">\n",
    "<img src=images/4.1.1.png width=\"400\" height=\"180\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Strategy and cost function\n",
    "Just like many other machine learning models, the strategy to pick up the optimal model is to minimize the cost function:\n",
    "$$ \\min_{\\mathbf{W}} \\sum_{i=1}^N (y_{i} - \\mathbf{W}^T\\mathbf{X}_{i})^2 $$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Algorithm\n",
    "\n",
    "#### 3.1 Normal equation\n",
    "By setting the derivatives to 0, we can use the following formula to solve the parameters:\n",
    "\n",
    "- ```The Formula:``` \n",
    "\n",
    "A mathematical equation that gives the result directly -- the Normal Equation:\n",
    "$$ \\hat{\\theta} = ({X^{T}X})^{-1} \\quad X^{T}y $$\n",
    "where $\\hat{\\theta}$ is the value of \\theta that minimizes the cost function and $y$ is the vector of target values.\n",
    "- ```Why Normal Equation gives us the result?```\n",
    "\n",
    "The idea is to minimize the cost function $J(\\theta)$,\n",
    "  $$ J(\\theta) = J({\\theta}_0,...{\\theta}_n) = \\frac{1}{2m}\\sum_{i=1}^{m}({\\theta}^{T} X_i-y_i)^2 $$\n",
    "  $$ = \\frac{1}{2m}(X_i\\theta-y_i)^{T}(X_i\\theta-y_i) $$\n",
    "  $$ = \\frac{1}{2m}({\\theta}^{T}{X}^{T}-{y_i}^{T})(X_i\\theta-y_i) $$\n",
    "  $$ = \\frac{1}{2m}({\\theta}^{T}{X}^{T}{X}{\\theta}-(X\\theta)^Ty_i-{y_i}^{T}X{\\theta}+{y_i}^{T}y_i) $$\n",
    "  $$ = \\frac{1}{2m}({\\theta}^{T}{X}^{T}{X}{\\theta}-2(X\\theta)^Ty_i+{y_i}^{T}y_i) $$\n",
    "  here, to minimize the cost function, we set partial derivatives to 0:\n",
    "  $$ \\frac{\\partial}{\\partial \\theta}J(\\theta)=\\frac{1}{2m}({\\theta}^{T}{X}^{T}{X}{\\theta}-2(X\\theta)^Ty_i+{y_i}^{T}y_i)=0 $$\n",
    "  $$ \\frac{\\partial}{\\partial \\theta}J(\\theta)=\\frac{1}{2m}\\frac{\\partial}{\\partial \\theta}({\\theta}^{T}{X}^{T}{X}{\\theta}-2{y_i}^{T}X\\theta) $$\n",
    "  $$ = \\frac{1}{2m}(X^{T}X\\theta+X^{T}X\\theta-2X^{T}y_i) $$\n",
    "  $$ = \\frac{1}{2m}(2X^{T}X\\theta-2X^{T}y_i) $$\n",
    "  when setting the partial derivatives to 0, we will get:\n",
    "  $$ X^{T}X\\theta = X^{T}y_i $$\n",
    "  multiply $(X^{T}X)^{-1}$ at both side:\n",
    "  $$ \\theta = (X^{T}X)^{-1}X^{T}y_i $$ \n",
    "\n",
    "- ```Pseudo-inverse:```\n",
    "The main reason that we cannot solve the optimization problem is that not all matrix has its inverse matrix; however, with some transformation, we can find the pseudo-inverse matrix ($(X^{T}X)^{-1}X^{T}$) for each matrix.\n",
    "\n",
    "#### 3.2 Gradient descent\n",
    "- Gradient Descent is guaranteed to approach arbitrarily close the gloabl minimum (with approriate learning rate):\n",
    "  - The MSE cost function for a Linear Regression model happens to be a convex function: there are no local minima, just one gloabl minimum. \n",
    "  - It is also a continuous function with a slope that never change abruptly. \n",
    "- Batch Gradient Descent uses the whole batch of training data at every step. As a result it is terribly slow on every large training sets.\n",
    "  - Partial derivatives of the cost function:\n",
    "$$ J(\\theta) = J({\\theta}_0,...{\\theta}_n) = \\frac{1}{2m}\\sum_{i=1}^{m}({\\theta}^{T} X-y_i)^2 $$\n",
    "$$ \\frac{\\partial}{\\partial {\\theta}_j}J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}({\\theta}^{T}x^{(i)}-y^{(i)})x_{j}^{(i)} $$\n",
    "$$ = \\frac{1}{m}(X^{T}X\\theta-X^{T}y_i) $$\n",
    "$$ = \\frac{1}{m}X^{T}(X\\theta-y_i) $$\n",
    "  - Once we have the gradient vector, which points uphill, just go in the opposite direction to go downhill:\n",
    "$$ {\\theta}_j := {\\theta}_j - \\alpha\\frac{\\partial}{\\partial {\\theta}_j}J(\\theta) $$\n",
    "- How to select approriate learning rate:\n",
    "  - We can try an alpha and see if the new $\\theta$ will decrease the cost function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Pros and cons\n",
    "- Pros: \n",
    "  - often useful out of the box;\n",
    "  - easy to understand.\n",
    "- Cons;\n",
    "  - danger of overfitting;\n",
    "  - sensitive to outliers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. A proof question:\n",
    "\n",
    "#### 5.1 Some background knowledge:\n",
    "- The cost function of a general linear regression:\n",
    "$$ J(\\theta) = J({\\theta}_0,...{\\theta}_n) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-{\\theta}^{T}h(x_i))^2 $$\n",
    "- The math equation of the Gaussian distribution:\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-{\\frac{1}{2}}(\\frac{x-\\mu}{\\sigma})^2} $$\n",
    "- The Maximum Likelihood Estimation:\n",
    "  - A method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "  - The idea is to transfer solving the density function directly to solveing parameters for the Likelihood Function (the product of univariate density function with unknown parameters to be solved for independent and identically distributed random variables).\n",
    "  - The general likelihood function:\n",
    "$$ L(\\theta) = f(x_1,...,x_n|\\theta) = \\prod_{i=1}^n f(x_i|\\theta) $$\n",
    "$$ log(L(\\theta)) = log(\\underset{\\theta} {\\textrm{arg max}} \\prod_{i=1}^n f(x_i|\\theta)) $$\n",
    ", where\n",
    "$$ \\theta = {\\theta_1, \\theta_2,..., \\theta_i} $$\n",
    "To solve $\\theta$, we write the partial derivatives for each parameters and let them equal to 0, then solve the parameters.\n",
    "$$\\frac{\\partial}{\\partial {\\theta_i}} = 0 $$\n",
    "  - ```Why we use 'log' (we usually use the natural log): ```\n",
    "When $a$ in ${log_a}()$ is greater than 1, the $x$ can be limitlessly large when the slope $k$ equals 0, that is, when the slope $k = 0$, $y$ can get its maximum value.\n",
    "\n",
    "#### 5.2 Proof:\n",
    "- Now we assume that $y = {\\theta}^{T}h(x_i) + \\epsilon$, where $\\epsilon$ ~ $N(0, {\\sigma}^2)$ ; this implies that $y|x_i$ ~ $ N({\\theta}^{T}h(x_i), {\\sigma}^2) $\n",
    "- The likelihood function (for all data) can be expressed as:\n",
    "  $$ L(y_1,...,y_i, x_1,...,x_i|\\theta) = \\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-{\\frac{1}{2}}(\\frac{y_i-{\\theta}^{T}h(x_i)}{\\sigma})^2} $$\n",
    "  here, we want to solve the parameters, $\\theta ( = {\\theta_1, \\theta_2,..., \\theta_i})$, that can make this model (distribution) most probable. Traditionally, to maximize the likelihood function, we should let the slope $k$ of $log(L(\\theta))$ equal to 0, that is, let the partial derivative of $log(L(\\theta))$ equal to 0. \n",
    "- We can also maximize the output of the likelihood function by minimizing its negative log function, $-log(L(\\theta))$:\n",
    "  $$ \\max \\quad log(L(\\theta)) = log(\\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-{\\frac{1}{2}}(\\frac{y_i-{\\theta}^{T}h(x_i)}{\\sigma})^2}) $$\n",
    "  is equivalent to\n",
    "  $$ \\min -log(L(y|x_i)) = - log(\\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-{\\frac{1}{2}}(\\frac{y_i-{\\theta}^{T}h(x_i)}{\\sigma})^2}) $$\n",
    "  $$ = \\sum_{i=1}^n \\frac{1}{2}(\\frac{y_i-{\\theta}^{T}h(x_i)}{\\sigma})^2 + \\sum_{i=1}^n log(\\sigma \\sqrt{2\\pi}) $$\n",
    "  $$ = \\sum_{i=1}^n \\frac{({{y_i}-{\\theta}^{T}h(x_i)})^2}{2{\\sigma}^2} + \\sum_{i=1}^n log(\\sigma \\sqrt{2\\pi}) $$\n",
    "  $$ = \\sum_{i=1}^n \\frac{({{y_i}-{\\theta}^{T}h(x_i)})^2}{2{\\sigma}^2} + \\sum_{i=1}^n log(\\sqrt{2\\pi}) + \\sum_{i=1}^n log \\sigma  $$\n",
    "- As $\\frac{1}{{\\sigma}^2}$ and $\\sum_{i=1}^n log(\\sigma \\sqrt{2\\pi})$ are constant (we know the value of $\\sigma$), to maximize the likelihood function, we only need to minimize \n",
    "  $$\\sum_{i=1}^n \\frac{1}{2}({{y_i}-{\\theta}^{T}h(x_i)})^2$$\n",
    ", the same as the cost function, $\\frac{1}{2}\\sum_{i=1}^{n}(y_i-{\\theta}^{T}h(x_i))^2 $\n",
    "-  Therefore, we can say that minimizing the cost function is equivalent to maximizing the likelihood of our model, regardless of the value of $\\sigma$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}