{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. SVMs overview:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 What does SVM learn from the training dataset and labeled data?\n",
    "- A linear model, or a line / hyperplane (for multiple variables);\n",
    "- Now, we have the equation that represents the 'line':\n",
    "$$ y = \\mathbf{w^{T}x} + w_0 $$\n",
    "- We use an algorithm to determine which are the values of W and b giving the 'best' line seperating the data;\n",
    "- SVM is one of the algorithms that help determine the two parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.2 Some background knowledge about SVM\n",
    "- SVMs include SVM (for classification) and SVR (for regression);\n",
    "- Four different SVM:\n",
    "  - The original one : the Maximal Margin Classifier,\n",
    "  - The kernelized version using the Kernel Trick,\n",
    "  - The soft-margin version,\n",
    "  - The soft-margin kernelized version (which combine 1, 2 and 3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Understanding the Math of SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.1 The Margin (concept)\n",
    "1. ```The goal of SVM:```\n",
    "The goal of a support vector machine is to find  the optimal separating hyperplane which maximizes the margin of the training data. \n",
    "2. ```Optimal seperating hyperplane:```The fact that you can find a separating hyperplane,  does not mean it is the best one !\n",
    "<p align=\"center\">\n",
    "<img src=https://www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated-2.png width=\"300\" height=\"300\" alt=\"SVM\" align=center>\n",
    "\n",
    "So we will try to select an hyperplane as far as possible from data points from each category. The optimal seperating hyperplane should:\n",
    "- correctly classifies the training data;\n",
    "- generalize better with unseen data.\n",
    "3. ```Margin:``` the optimal hyperplane will be the one with the biggest margin."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.2 Margin Calculation\n",
    "<p align=\"center\">\n",
    "<img src=images/SVM1.jpg width=\"200\" height=\"200\" alt=\"SVM1\" align=center>\n",
    "<img src=images/SVM2.jpg width=\"200\" height=\"200\" alt=\"SVM2\" align=center>\n",
    "\n",
    "- The distance of one training data (x) to the hyperplane is c, which is equal to |b-a|, while the margin is the distance from the closest training point to the hyperplane, minimize $c$;\n",
    "\n",
    "- Step 1: calculating b\n",
    "  - z (a vector) has the magnitude of b; its direction is the same as $w$, so its direction would be $\\frac{\\mathbf{w}}{||\\mathbf{w}||}$;\n",
    "  - That is, $z = b\\frac{\\mathbf{w}}{||\\mathbf{w}||}$;\n",
    "  - z on the hyperplane, so we have $ \\mathbf{w^Tz} + w_0 = 0$;\n",
    "  $$ \\mathbf{w^T} \\frac{b\\mathbf{w}}{||\\mathbf{w}||} + w_0 = 0 $$\n",
    "  $$ b||\\mathbf{w}|| + w_0 = 0 $$\n",
    "  $$ b = - \\frac{w_0}{||\\mathbf{w}||} $$\n",
    "  Note: $||\\mathbf{w}|| = \\sqrt{\\mathbf{w^Tw}}$\n",
    "- Step 2: calculating a\n",
    "  - $a$ is the magnituede of $x$'s projection on $w$;\n",
    "  - that is, \n",
    "  $$a = \\frac {\\mathbf{w^Tx}}{||\\mathbf{w}||}$$\n",
    "- Step 3: calculating c\n",
    "  - $ c = |b-a| = |\\frac{w_0}{||\\mathbf{w}||} + \\frac {\\mathbf{w^Tx}}{||\\mathbf{w}||}| $\n",
    "  - that is, the distance of one training point \n",
    "  $$ c = \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx} + w_0| $$\n",
    "- Step 4: the margin\n",
    "  - therefore, the margin is \n",
    "  $$ \\underset{i}{\\text{minimize}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/margin1.jpg width=\"400\" height=\"300\" alt=\"SVM1\" align=center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.3 The SVM optimisation problem\n",
    "- Scaling: $(\\mathbf{w}, w_0)$ and $(c\\mathbf{w}, cw_0)$ define the same hyperplane;\n",
    "- This is because: $c\\mathbf{w^Tx} + cw_0 \\geq 0$ is equal to $\\mathbf{w^Tx} + w_0 \\geq 0$;\n",
    "- Put a constraint on $(\\mathbf{w}, w_0)$, \n",
    "$$ \\underset{i}{\\text{min}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| = 1 $$\n",
    "- Now the margin will always be $\\frac{1}{||\\mathbf{w}||}$;\n",
    "- We want a hyperplane that will maximize the margin:\n",
    "$$ \\underset{\\mathbf{w}}{\\text{max}} \\frac{1}{||\\mathbf{w}||} $$\n",
    "subject to: \n",
    "$$\\mathbf{w^Tx_i} + w_0 \\geq 1 $$, for all i with $y_i = 1$; \n",
    "$$\\mathbf{w^Tx_i} + w_0 \\leq -1 $$, for all i with $y_i = -1$; \n",
    "$$ \\underset{i}{\\text{min}} \\frac{1}{||\\mathbf{w}||}|\\mathbf{w^Tx_i} + w_0| = 1 $$\n",
    "- After deleting the third rebundent restriction and simplifying the first two restrictions, we have:\n",
    "$$ \\underset{\\mathbf{w}}{\\text{max}} \\frac{1}{2||\\mathbf{w}||} $$\n",
    "subject to:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i\n",
    "- The above optimization is equal to:\n",
    "$$ \\underset{i}{\\text{min}} \\frac{1}{2}||\\mathbf{w}||^2 $$\n",
    "subject to:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.4 The solution of optimal paramters\n",
    "- ```Compute w:```\n",
    "$$ \\mathbf{w} = \\sum_i {\\alpha}_i{y_i}{\\mathbf{x_i}} $$\n",
    "- ```Compute w_0:```\n",
    "  - we can use a constraint to calculate $w_0$:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) = 1 $$\n",
    "  - multiply $y_i$ at each side (note ${y_i}^2 = 1$),\n",
    "$$ \\mathbf{w^Tx_i} + w_0 = y_i $$\n",
    "$$ w_0 = y_i - \\mathbf{w^Tx_i} $$\n",
    "- ```Hypothesis function:```\n",
    "  - therefore, prediction on new data point $x$ is:\n",
    "$$ f(x) = \\text{sign}((\\mathbf{w^Tx}) + w_0) $$\n",
    "$$ = \\text{sign}(\\sum_i^n {\\alpha}_i{y_i}({\\mathbf{x_i}^T}\\mathbf{x}) + w_0)$$\n",
    "- The formulation of the SVM is the hard margin SVM. It can not work when the data is not linearly separable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Soft Margin SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.1 When and how soft margin SVM helps?\n",
    "<p align=\"center\">\n",
    "<img src=images/Soft_SVM1.jpg width=\"200\" height=\"200\" alt=\"SVM3\" align=center>\n",
    "<img src=images/Soft_SVM2.jpg width=\"200\" height=\"200\" alt=\"SVM4\" align=center>\n",
    "\n",
    "- Outlier reducing the margin vs. outlier breaking linear separability\n",
    "- Soft margin SVM allows mistakes, but should make as few mistakes as possible."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.2 $\\zeta$:\n",
    "- Therefore, we need to modify the constraints of the optimization problem, from...to...:\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 $$, for all i\n",
    "$$ y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i $$, for all i\n",
    "- However, if we choose a very large $\\zeta$, the constraint can be satisfied quite easily. To keep the mistakes as few as possible, we can modify the objective function to penalize the choice of $\\zeta$. That is,\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{minimize}} \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^m \\zeta_i $$\n",
    "\n",
    "$$ \\text{subject to} \\quad y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i \\quad \\text{where} \\quad \\zeta_i \\geq 0 \\quad \\text{for any i=1,...,m} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.3 $C$:\n",
    "Generally speaking, parameter C will help us to determine how important the $\\zeta$ should be.\n",
    "<p align=\"center\">\n",
    "<img src=images/Soft_margin1.jpg width=\"500\" height=\"200\" alt=\"SVM5\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Soft_margin2.jpg width=\"500\" height=\"200\" alt=\"SVM6\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/Soft_margin3.jpg width=\"500\" height=\"200\" alt=\"SVM7\" align=centering>\n",
    "\n",
    "  - 1) small C --> wider margin --> cost of some misclassifications;\n",
    "  - 2) big C --> hard margin --> no tolerance of misclassifications;\n",
    "  - 3) no magic value for C --> select C by grid search with cross-validation (note: C is specific to what we are using)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4 2-Norm soft margin (L2 regularized)\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{minimize}} \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^m \\zeta_i^2 $$\n",
    "\n",
    "$$ \\text{subject to} \\quad y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i \\quad \\text{where} \\quad \\zeta_i \\geq 0 \\quad \\text{for any i=1,...,m} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Kernals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1 Feature mapping\n",
    "- In some case, the data is not linearly seperable, such as:\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal1.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "\n",
    "- Just as what we do in polynomial regression, we can do polynomial mapping to make: \n",
    "$$ \\phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3 $$\n",
    "defined by,\n",
    "$$ \\phi(x_1, x_2) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2) $$\n",
    "- Now, the graph becomes:\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal2.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal3.jpg width=\"250\" height=\"200\" alt=\"SVM8\" align=centering>\n",
    "\n",
    "- However, we have to try which transformation to apply dependent on the data that we have. [sklearn dataset transformation](https://scikit-learn.org/stable/data_transforms.html)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/decisionboundary.jpg width=\"225\" height=\"200\" alt=\"SVM8\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.2 What is a kernel?\n",
    "- ```Definition: ```\n",
    "A kernel function is a function that computes the ```similarity``` between two vectors (or points...). Although there are many types of kernel functions, the main aim of this function is to the degree of similarity between two vectors (or points...).\n",
    "- ```General form: ```\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\langle{\\phi(\\mathbf{x}), \\phi(\\mathbf{x'})}\\rangle_\\mathcal{V} $$\n",
    "- ```General rule: ```\n",
    "Try a RBF kernel first, because it uasually works well.\n",
    "\n",
    "- ```Linear Kernel: ```\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\mathbf{x}^T·\\mathbf{x'} $$\n",
    "Linear kernel is also called a kernel function without a kernel. It simply computes the dot product of two vectors. The dot product of two vectors is also a measurement of similarity. \n",
    "\n",
    "- ```Polynomial Kernel: ```\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = (\\mathbf{x}^T·\\mathbf{x'} + w_0)^d $$\n",
    "Polynomial kernels can help with situations when a curve decision boundary is needed. In addition, compared with feature mapping, the polynomial kernels can reduce computation (explained in details later).\n",
    "(```Note: ```using a high-degree polynomial kernal will often lead to overfitting)\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal4.jpg width=\"500\" height=\"200\" alt=\"SVM9\" align=centering>\n",
    "\n",
    "- ```RBF or Gaussian kernel: ```\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\text{exp}(-\\gamma||\\mathbf{x} - \\mathbf{x'}||^2) $$\n",
    ", or\n",
    "$$ K(\\mathbf{x}, \\mathbf{x'}) = \\text{exp}(-\\frac{||\\mathbf{x} - \\mathbf{x'}||^2}{2{\\sigma}^2}) $$\n",
    "- the RBF(Radial Basis Function) returns the result of a dot product performed in $\\mathbb{R}^{\\infty} $\n",
    "- the RBF is also a good way to compute the similarity between two vectors: when two vectors are quite close, the function will infinitely be close to 1; when they are really far from each other, the function will be very close to 0;\n",
    "- do perform feature scaling before the Gaussian kernel;\n",
    "- [How to choose gamma with sklearn](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal6.jpg width=\"300\" height=\"200\" alt=\"SVM11\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/kernal5.jpg width=\"500\" height=\"200\" alt=\"SVM10\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.3 Why we need a kernel function?\n",
    "1. The feature mapping transformation will transform every example. It will take a huge amount of time to do it when we have millions of examples. Kernel does not need to transform every exmaple, we can compare the following two functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8100.0\n8100\n"
     ]
    }
   ],
   "source": [
    "# Transform a two-dimensional vector x into a three-dimensional vector\n",
    "import numpy as np \n",
    "def transform(x): \n",
    "    return [x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2]\n",
    "def polynomial_kernel(a, b): \n",
    "    return a[0]**2 * b[0]**2 + 2*a[0]*b[0]*a[1]*b[1] + a[1]**2 * b[1]**2\n",
    "\n",
    "x1 = [3,6] \n",
    "x2 = [10,10] \n",
    "x1_3d = transform(x1) \n",
    "x2_3d = transform(x2)\n",
    "print(np.dot(x1_3d,x2_3d))\n",
    "print(polynomial_kernel(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8100\n"
     ]
    }
   ],
   "source": [
    "def polynomial_kernel(a, b, degree, constant=0): \n",
    "    result = sum([a[i] * b[i] for i in range(len(a))]) + constant   \n",
    "    return pow(result, degree)\n",
    "# We do not transform the data\n",
    "print(polynomial_kernel(x1, x2, degree=2))"
   ]
  },
  {
   "source": [
    "2. Kernal trick:\n",
    "Another main reason that we can use kernal functions is that they compute similarities, which is exactly the original hypothesis functions are doing:\n",
    "- The original hypothesis function: \n",
    "    \n",
    "$$ h(\\mathbf{x}) = \\text{sign}((\\mathbf{w^Tx}) + w_0) $$\n",
    "$$ = \\text{sign}(\\sum_i^n {\\alpha}_i{y_i}(\\mathbf{x_i^T}\\mathbf{x}) + w_0)$$\n",
    "or,\n",
    "$$ h(\\mathbf{x}) = \\text{sign}(\\sum_i^n {\\lambda_i}(\\mathbf{x_i^T}\\mathbf{x}) + w_0) $$\n",
    "\n",
    "- With a kernal (note that SVM is a sparse kernal machines):\n",
    "$$ h(\\mathbf{x}) = \\text{sign}(\\sum_i^n {\\lambda_i}K(\\mathbf{x_i}, \\mathbf{x}) + w_0) $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/prediction.jpg width=\"400\" height=\"250\" alt=\"SVM16\" align=centering>\n",
    "\n",
    "Figure Credit: Bernhard Schoelkopf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. Logistics regression vs. SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- ```Logistics regression optimisation problem:```\n",
    "$$ \\underset{\\theta}{\\text{min}} \\frac{1}{m} [\\sum_{i=1}^m y^{(i)}(-\\text{log}h_{\\theta}(x^{(i)}))+(1-y^{(i)})(-\\text{log}(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2m}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    ", or\n",
    "$$ \\underset{\\theta}{\\text{min}}[\\sum_{i=1}^m y^{(i)}(-\\text{log}h_{\\theta}(x^{(i)}))+(1-y^{(i)})(-\\text{log}(1-h_{\\theta}(x^{(i)})))]+\\frac{\\lambda}{2}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    ", that is, \n",
    "$$ \\underset{\\theta}{\\text{min}}[\\sum_{i=1}^m y^{(i)}(-\\text{log}\\frac{1}{1 + e^{-{\\theta}^Tx^{(i)}}}+(1-y^{(i)})(-\\text{log}(1-\\frac{1}{1 + e^{-{\\theta}^Tx^{(i)}}})]+\\frac{\\lambda}{2}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    ", which is equal to,\n",
    "$$ \\underset{\\theta}{\\text{min}}\\sum_{i=1}^m \\text{log}(1 + e^{- y_{i}{\\theta}^Tx^{(i)}})+\\frac{\\lambda}{2}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    ", let $C=\\frac{1}{\\lambda}$\n",
    "$$ \\underset{\\theta}{\\text{min}} C\\sum_{i=1}^m \\text{log}(1 + e^{- y_{i}{\\theta}^Tx^{(i)}})+\\frac{1}{2}||\\mathbf{w}||^2 $$ \n",
    "$$ \\underset{\\theta}{\\text{min}} C\\sum_{i=1}^m \\text{log}(1 + e^{- y_{i}f(\\mathbf{x_i})})+\\frac{1}{2}||\\mathbf{w}||^2 $$ \n",
    "\n",
    "- ```Support vector machine:```\n",
    "$$ \\underset{\\theta}{\\text{min}} C\\sum_{i=1}^m [y^{(i)}\\text{cost}_1({\\theta}^T(x^{(i)})+(1-y^{(i)})(\\text{cost}_0{\\theta}^T(x^{(i)})]+\\frac{1}{2}\\sum_{j=1}^n{\\theta_j}^2 $$\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{min}} C\\sum_{i=1}^m \\zeta_i + \\frac{1}{2} ||\\mathbf{w}||^2 $$\n",
    "$$ \\text{subject to} \\quad y_i(\\mathbf{w^Tx_i} + w_0) \\geq 1 - \\zeta_i \\quad \\text{where} \\quad \\zeta_i \\geq 0 \\quad \\text{for any i=1,...,m} $$\n",
    ", so that we have,\n",
    "$$ \\underset{\\mathbf{w},b,\\zeta}{\\text{min}} C\\sum_{i=1}^m \\text{max}(0, 1-y_{i}f(\\mathbf{x_i})) + \\frac{1}{2} ||\\mathbf{w}||^2 $$\n",
    ", where $f(\\mathbf{x_i}) = \\mathbf{w^Tx_i} + w_0$ \n",
    "\n",
    "- ```Two hinge functions:```\n",
    "$$ g_{hinge}(z) = \\text{log}(1 + e^{-z}) $$\n",
    "$$ g_{hige}(z) = \\text{max}(0, 1-z) $$\n",
    "c"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Multi-Class SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6.1 One-against-all\n",
    "We can transform the n classes problem to n binary classes problems. Just like the following code is doing:\n",
    "<p align=\"center\">\n",
    "<img src=images/multiclassification1.jpg width=\"200\" height=\"200\" alt=\"SVM12\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def load_X(): \n",
    "    return np.array([[1, 6], [1, 7], [2, 5], [2, 8], [4, 2], [4, 3], [5, 1], [5, 2], [5, 3], [6, 1], [6, 2], [9, 4], [9, 7], [10, 5], [10, 6], [11, 6], [5, 9], [5, 10], [5, 11], [6, 9], [6, 10], [7, 10], [8, 11]]) \n",
    "def load_y(): \n",
    "    return np.array([1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4])\n",
    "\n",
    "from sklearn import svm \n",
    "# Create a simple dataset \n",
    "X = load_X() \n",
    "y = load_y() \n",
    "# Transform the 4 classes problem \n",
    "# in 4 binary classes problems. \n",
    "y_1 = np.where(y == 1, 1, -1) \n",
    "y_2 = np.where(y == 2, 1, -1) \n",
    "y_3 = np.where(y == 3, 1, -1) \n",
    "y_4 = np.where(y == 4, 1, -1)\n",
    "\n",
    "# Train one binary classifier on each problem. \n",
    "y_list = [y_1, y_2, y_3, y_4] \n",
    "classifiers = [] \n",
    "for y_i in y_list: \n",
    "    clf = svm.SVC(kernel='linear', C=1000) \n",
    "    clf.fit(X, y_i) \n",
    "    classifiers.append(clf)"
   ]
  },
  {
   "source": [
    "<p align=\"center\">\n",
    "<img src=images/multiclassification2.jpg width=\"300\" height=\"300\" alt=\"SVM12\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "def predict_class(X, classifiers): \n",
    "    predictions = np.zeros((X.shape[0], len(classifiers))) \n",
    "    for idx, clf in enumerate(classifiers): \n",
    "        predictions[:, idx] = clf.predict(X) \n",
    "    # returns the class number if only one classifier predicted it \n",
    "    # returns zero otherwise. \n",
    "    return np.where((predictions == 1).sum(1) == 1, \n",
    "                    (predictions == 1).argmax(axis=1) + 1, 0)\n",
    "\n",
    "predict_class(np.array([[1,6]]), classifiers)"
   ]
  },
  {
   "source": [
    "<p align=\"center\">\n",
    "<img src=images/multiclassification3.jpg width=\"300\" height=\"300\" alt=\"SVM13\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Notice that we cannot decide which class points in blue zones should be claasified to. An alternative solution is to classify points to the class that gives the highest value of the decision function, that is:\n",
    "<p align=\"center\">\n",
    "<img src=images/multiclassification4.jpg width=\"300\" height=\"200\" alt=\"SVM13\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, classifiers): \n",
    "    predictions = np.zeros((X.shape[0], len(classifiers))) \n",
    "    for idx, clf in enumerate(classifiers): \n",
    "        predictions[:, idx] = clf.decision_function(X) \n",
    "        # return the argmax of the decision function as suggested by Vapnik. \n",
    "        return np.argmax(predictions, axis=1) + 1"
   ]
  },
  {
   "source": [
    "from sklearn.svm import LinearSVC \n",
    "import numpy as np \n",
    "X = load_X() \n",
    "y = load_y() \n",
    "clf = LinearSVC(C=1000, random_state=88, multi_class='ovr') \n",
    "clf.fit(X,y) \n",
    "# Make predictions on two examples. \n",
    "X_to_predict = np.array([[1, 6]]) \n",
    "print(clf.predict(X_to_predict)) \n",
    "# prints [1 6]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "However, this classification method has two potential problems:\n",
    "- scale;\n",
    "- imbalanced training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6.2 One-against-one \n",
    "We train one classifier per pair of classes, leading to $K(K-1)/2$ classifiers for $K$ classes. Predictions are made using a simple voting strategy: pass each new instance to each classifier and record the predicted. The class having the most votes is assigned to the example.\n",
    "<p align=\"center\">\n",
    "<img src=images/classificatonoao1.jpg width=\"300\" height=\"200\" alt=\"SVM14\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/classificationoao2.jpg width=\"250\" height=\"200\" alt=\"SVM15\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/classificationoao3.jpg width=\"300\" height=\"200\" alt=\"SVM16\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations \n",
    "from scipy.stats import mode \n",
    "from sklearn import svm \n",
    "import numpy as np \n",
    "# Predict the class having the max number of votes. \n",
    "def predict_class(X, classifiers, class_pairs): \n",
    "    predictions = np.zeros((X.shape[0], len(classifiers))) \n",
    "    for idx, clf in enumerate(classifiers): \n",
    "        class_pair = class_pairs[idx] \n",
    "        prediction = clf.predict(X) \n",
    "        predictions[:, idx] = np.where(prediction == 1, class_pair[0], class_pair[1]) \n",
    "        return mode(predictions, axis=1)[0].ravel().astype(int) \n",
    "\n",
    "X = load_X() \n",
    "y = load_y() \n",
    "# Create datasets. \n",
    "training_data = [] \n",
    "class_pairs = list(combinations(set(y), 2)) \n",
    "for class_pair in class_pairs: \n",
    "    class_mask = np.where((y == class_pair[0]) | (y == class_pair[1])) \n",
    "    y_i = np.where(y[class_mask] == class_pair[0], 1, -1)\n",
    "    training_data.append((X[class_mask], y_i)) \n",
    "\n",
    "# Train one classifier per class. \n",
    "classifiers = [] \n",
    "for data in training_data: \n",
    "    clf = svm.SVC(kernel='linear', C=1000) \n",
    "    clf.fit(data[0], data[1]) \n",
    "    classifiers.append(clf) \n",
    "# Make predictions on two examples. \n",
    "X_to_predict = np.array([[5,5],[2,5]]) \n",
    "print(predict_class(X_to_predict, classifiers, class_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "import numpy as np \n",
    "X = load_X() \n",
    "y = load_y() # Train a multi-class classifier. \n",
    "clf = svm.SVC(kernel='linear', C=1000) \n",
    "clf.fit(X,y) \n",
    "# Make predictions on two examples. \n",
    "X_to_predict = np.array([[5,5],[2,5]]) \n",
    "print(clf.predict(X_to_predict)) # prints [2 1]"
   ]
  },
  {
   "source": [
    "One of the main drawbacks of the one-against-all method is that the classifier will tend to overfit. Moreover, the number of classfiers that we need to build can be really large when we have a huge number of classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6.3 DAGSVM (Directed Acyclic Graph SVM)\n",
    "Following certain path, for a problem with K classes, K-1 decision nodes will be evaluated.\n",
    "<p align=\"center\">\n",
    "<img src=images/DAGSVM.jpg width=\"250\" height=\"200\" alt=\"SVM5\" align=centering>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, classifiers, distinct_classes, class_pairs):\n",
    "    results = [] \n",
    "    for x_row in X: \n",
    "        class_list = list(distinct_classes) \n",
    "        # After each prediction, delete the rejected class \n",
    "        # until there is only one class. \n",
    "        while len(class_list) > 1: \n",
    "            # We start with the pair of the first and \n",
    "            # last element in the list. \n",
    "            class_pair = (class_list[0], class_list[-1])   \n",
    "            classifier_index = class_pairs.index(class_pair) \n",
    "            y_pred = classifiers[classifier_index].predict(x_row) \n",
    "            \n",
    "            if y_pred == 1: \n",
    "                class_to_delete = class_pair[1] \n",
    "            else: class_to_delete = class_pair[0] \n",
    "                class_list.remove(class_to_delete) \n",
    "        \n",
    "        results.append(class_list[0]) \n",
    "    return np.array(results)"
   ]
  },
  {
   "source": [
    "### 6.4 Solving a single optimization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "import numpy as np \n",
    "X = load_X() \n",
    "y = load_y() \n",
    "clf = svm.LinearSVC(C=1000, multi_class='crammer_singer') \n",
    "clf.fit(X,y) # Make predictions on two examples. \n",
    "X_to_predict = np.array([[5,5],[2,5]]) \n",
    "print(clf.predict(X_to_predict)) # prints [4 1]"
   ]
  },
  {
   "source": [
    "## References:\n",
    "- Kowalczyk, A., 2017, Support Vector Machines Succinctly, pp.65-102, Syncfusion. Available at: https://www.syncfusion.com/\n",
    "- The University of Edinburgh, IAML Week 6 Lecture notes.\n",
    "- Andew, Ng., Support Vector Machines, Machine Learning, Coursera."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}