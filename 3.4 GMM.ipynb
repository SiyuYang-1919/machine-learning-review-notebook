{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Model\n",
    "$$ p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k}) $$\n",
    ", where $\\pi_{k}$ is the mixing coefficient (we assume these Gaussian models are linearly mixed), $\\sum_{k=1}^K \\pi_{k} = 1$, $0<=\\pi_{k}<=1$; $N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k})$ is the component.\n",
    "$$ N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k}) = \\frac{1}{\\sqrt{2\\pi \\sigma_{k}^2}} e^{-\\frac{(y-\\mu_{k})^2}{2\\sigma_{k}^2}} $$\n",
    "\n",
    "<center class=\"half\">\n",
    "<img src=\"images/3.4.1.png\" width=\"300\"/>\n",
    "</center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Strategy\n",
    "The likelihood function:\n",
    "$$ P(\\mathbf{x}|\\mathbf{\\theta}) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k}) $$\n",
    "so the log likelihood function is:\n",
    "$$ P(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_{i=1}^n log \\sum_{k=1}^K \\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k}) $$\n",
    "The strategy is to maximize the likelihood of the function above. However, it would be difficult to use maximum likelihood extimation, as there is summation in log function. Therefore, EM algorithm is used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Using EM Algorithm to estimate parameters\n",
    "- Add latent variable:\n",
    "$$ \n",
    "\\gamma_{jk} = \n",
    "\\begin{cases}\n",
    "1,\\quad \\text{the }j_{th} \\text{observation is from} k_{th} \\text{component model}\\\\\n",
    "0, \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    ", where $ j=1,2,...,N; k=1,2,...,K $\n",
    "\n",
    "- New likelihood function:\n",
    "$$ P(\\mathbf{x}|\\gamma) = \\prod_{k=1}^K \\prod_{j=1}^N [\\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k})]^{\\gamma_{jk}} $$ \n",
    "$$ = \\prod_{k=1}^K \\pi_{k}^{n_{k}} \\prod_{j=1}^N [\\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k})]^{\\gamma_{jk}} $$\n",
    "$$ = \\prod_{k=1}^K \\pi_{k}^{n_{k}} \\prod_{j=1}^N [\\frac{1}{\\sqrt{2\\pi \\sigma_{k}^2}} e^{-\\frac{(y-\\mu_{k})^2}{2\\sigma_{k}^2}}]^{\\gamma_{jk}} $$\n",
    ", where $n_{k} = \\sum_{j=1}^N \\gamma_{jk}, \\sum_{k=1}^K n_{k} = N$\n",
    "\n",
    "- New log likelihood function:\n",
    "$$ log \\quad P(\\mathbf{x}|\\gamma) = \\sum_{k=1}^K \\left \\{n_{k}log\\pi_{k} + \\sum_{j=1}^N \\gamma_{jk} [log(\\frac{1}{\\sqrt{2\\pi}}) - log \\sigma_{k} - \\frac{1}{2\\sigma^{2}}(y_{i} - \\mu_{k})^2] \\right \\} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.1 E-step: Q function\n",
    "$$ Q(\\theta, \\theta^{(i)}) = E[log P(y,\\gamma|\\theta)|y,\\theta^{(i)}] = E \\left \\{\\sum_{k=1}^K \\left \\{n_{k}log\\pi_{k} + \\sum_{j=1}^N \\gamma_{jk} [log(\\frac{1}{\\sqrt{2\\pi}}) - log \\sigma_{k} - \\frac{1}{2\\sigma^{2}}(y_{i} - \\mu_{k})^2] \\right \\} \\right \\}$$\n",
    "$$ = \\sum_{k=1}^K \\left \\{\\sum_{j=1}^N (E\\gamma_{jk})log\\pi_{k} + \\sum_{j=1}^N (E\\gamma_{jk})[log(\\frac{1}{\\sqrt{2\\pi}}) - log \\sigma_{k} - \\frac{1}{2\\sigma^{2}}(y_{i} - \\mu_{k})^2] \\right \\} $$\n",
    "here, we calculate \n",
    "$$ \\hat{\\gamma_{jk}} = E(\\gamma_{jk}|y,\\theta) = \\frac{\\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k})}{\\sum_{k=1}^K \\pi_{k}N(\\mathbf{x}|\\mu_{k}, \\Sigma_{k})} $$, where $j = 1,2,..,N$ and $k = 1,2,...,K$\n",
    "$$n_{k} = \\sum_{j=1}^N E\\gamma_{jk}$$\n",
    ", so that we have\n",
    "$$ Q(\\theta, \\theta^{(i)}) = \\sum_{k=1}^K \\left \\{n_{k}log\\pi_{k} + \\sum_{j=1}^N \\hat{\\gamma_{jk}} [log(\\frac{1}{\\sqrt{2\\pi}}) - log \\sigma_{k} - \\frac{1}{2\\sigma^{2}}(y_{i} - \\mu_{k})^2] \\right \\}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.2 M-step: use interative method to update parameters\n",
    "$$ \\theta^{(i+1)} = \\arg \\max_{\\theta}Q(\\theta,\\theta^{(i)}) $$\n",
    "by solving the optimization problem, we will have:\n",
    "<center class=\"half\">\n",
    "<img src=\"images/3.4.2.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "Repeat the steps until the log likelihood function does not have any significant change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.3 Summary\n",
    "<center class=\"half\">\n",
    "<img src=\"images/3.4.3.png\" width=\"300\"/>  \n",
    "</center>\n",
    "<center class=\"half\">\n",
    "<img src=\"images/3.4.4.png\" width=\"300\"/>\n",
    "</center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}