{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Intution of PCA\n",
    "- ```Aim:``` construct a new set of dimensions ($m$) to represent original data with $d$ dimensions, where $m<<d$, that is, we want to change the coordinates of every data point to the $m$ dimensions.\n",
    "- ```What we need to know:```\n",
    "  - New coordinate of the origin, $O$;\n",
    "  - New angels of the coordinate system or the directions;\n",
    "  - New coordinate of each sample point.\n",
    "- ```Objective:``` reduce the dimensions while maintaining the most information\n",
    "  - The information amount of a pincipal conponent $y_{1}$ is measured by the variances of points projected to its direction, information amount is greater when the variances are larger (can also be measured by the sum of distances between original points and the direction)\n",
    "  - Therefore, to maintain the most information, we need to find the one with the largest variance and the smallest sum of distances (maximize the variance and minize the distances):\n",
    "  <p align=\"center\">\n",
    "<img src=images/3.3.1.png width=\"400\" height=\"300\" alt=\"5\" align=centering>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. PCA algorithm\n",
    "- Step 1: let the center of the sample data to be the new origin, subtract mean from each attribute\n",
    "$$ \\mathbf{x}_{i,j} = \\mathbf{x}_{i,j} - \\mu_{j} $$\n",
    "- Step 2: compute the covariance matrix\n",
    "$$ C^{'} = \\frac{1}{n-1} D^{'}D^{'T} $$\n",
    "We know that $ C^{'} = RLR^{-1} $ and we want to get $R$ and $L$\n",
    "$$ C^{'}R = RL $$\n",
    "- Step 3: find eigenvalues by $det(C^{'}-LI)=0$\n",
    "- Step 4: find $i_{th}$ eigenvector by solving $C^{'} R_{i} = L_{i} R_{i}$\n",
    "- Step 5: apply the principal component matrix to the original data:\n",
    "$$ Y = R^{T}X $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. 数据的线性变换（补充背景知识）\n",
    "- 拉伸：\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.2.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 旋转：\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.3.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 白数据：\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.4.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 需要降维的数据可以由白数据进行拉伸以及旋转得到：\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.5.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 反之，我们可以通过逆向拉伸和旋转把需要降维的数据变成白数据，其中$R^{-1}$等于$R^{T}$\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.6.png width=\"400\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 求R：R是协方差矩阵的特征向量\n",
    "- 协方差：表示的是两个变量在变化过程中的联系（变化方向和程度）\n",
    "$$ cov(\\mathbf{x},\\mathbf{y}) = \\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1} $$\n",
    "- 协方差矩阵：\n",
    "$\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   cov(\\mathbf{x_{1}},\\mathbf{x_{1}}) & cov(\\mathbf{x_{1}},\\mathbf{x_{2}}) & cov\\mathbf{x_{1}}\\mathbf{x_{3}}) ... & cov(\\mathbf{x_{1}},\\mathbf{x_{m}})\\\\\n",
    "   cov(\\mathbf{x_{2}},\\mathbf{x_{1}}) & cov(\\mathbf{x_{2}},\\mathbf{x_{2}}) & cov（\\mathbf{x_{2}}\\mathbf{x_{3}}) ... & cov(\\mathbf{x_{2}},\\mathbf{x_{m}})\\\\\n",
    "   ...&...&...&...\\\\\n",
    "   cov(\\mathbf{x_{m}},\\mathbf{x_{1}}) & cov(\\mathbf{x_{m}},\\mathbf{x_{2}}) & cov（\\mathbf{x_{m}}\\mathbf{x_{3}}) ... & cov(\\mathbf{x_{m}},\\mathbf{x_{m}})\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$\n",
    "$$ C = \\frac{1}{n-1}DD^T $$\n",
    "- 推导过程：\n",
    "因为有：$ D^{'} = RSD $ 和 $ C^{'} = \\frac{1}{n-1} D^{'} D^{'T} $\n",
    "所以可得：\n",
    "$$ C^{'} = \\frac{1}{n-1}RSD(RSD)^T $$\n",
    "$$ = \\frac{1}{n-1}RSDD^TS^TR^T $$\n",
    "$$ = RSCS^TR^T $$\n",
    "因为D是白数据，它的协方差矩阵其实是单位矩阵，可忽略\n",
    "所以：\n",
    "$$ C^{'} = RSS^TR^T $$\n",
    "S定义的是一个只有对角线上有数据的拉伸矩阵，所以$S = S^T$，我们令$L = SS^T$；且因为$R^{-1} = R^{T}$，所以可用$R^{-1}$替换$R^{T}$\n",
    "可得：\n",
    "$$ C^{'} = RLR^{-1} $$\n",
    "- 特征向量与特征值：\n",
    "$$C^{'}v = \\lambda v$$\n",
    "如果令$\\mathbf{V}$和$\\mathbf{\\lambda}$分别代表特征向量和特征值矩阵，可以写成：\n",
    "$$C^{'}\\mathbf{V} = \\mathbf{\\lambda}\\mathbf{V}$$\n",
    "$$C^{'} = \\mathbf{V} \\mathbf{\\lambda}\\mathbf{V^{-1}}$$\n",
    "对比以上公式$ C^{'} = RLR^{-1} $，我们可得\n",
    "$$L = \\mathbf{\\lambda} = SS^{T}$$\n",
    "即\n",
    "$$\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   \\lambda_{1} & 0 &...& 0\\\\\n",
    "   0 & \\lambda_{2} & ... & 0\\\\\n",
    "   ...&...&...&...\\\\\n",
    "   0 & 0 & ... & \\lambda_{m}\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  =\n",
    "  \\left[\n",
    " \\begin{matrix}\n",
    "   a_{1}^{2} & 0 &...& 0\\\\\n",
    "   0 & a_{2}^{2} & ... & 0\\\\\n",
    "   ...&...&...&...\\\\\n",
    "   0 & 0 & ... & a_{m}^{2}\\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "- $a_{m}^{2}$就是原数据投射至第$m$个主成分上后的方差，与$\\lambda_{m}$ （与第$m$个主成分，也就是第$m$个特征向量对应的特征值）相等\n",
    "- 再深入研究一下$ C^{'} = RLR^{-1} $：\n",
    "$ C^{'} $ 是需要降维的数据的协方差，$R$表示的它是特征向量矩阵，而$L$它是特征值矩阵，也同时是$D^{'}$在坐标系进行$R^{-1}$方向变换后的新坐标系上的协方差（此时任意两个维度上的数据都是正交的）\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.7.png width=\"400\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. SVD (补充)\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.8.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 拓展到矩阵层面\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.9.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 奇异值矩阵中的值是按从大到小排列的，可选择前几个来进行分解：\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.10.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- 求SVD分解\n",
    "$$ M = U\\Sigma V^{T} $$\n",
    "$$ M^{T}M = (U\\Sigma V^{T})^{T}U\\Sigma V^{T} $$\n",
    "$$ M^{T}M = V\\Sigma U^{T}U\\Sigma V^{T} $$\n",
    "$$ M^{T}M = V\\Sigma\\Sigma V^{T} $$\n",
    "令$\\Sigma\\Sigma = L$,\n",
    "$$ M^{T}M = VLV^{T} $$\n",
    "$$ M^{T}MV = VL $$\n",
    "$$ MM^{T}U = UL $$\n",
    "由特征向量和特征值的公式：\n",
    "$$ D\\mathbf{V} = \\mathbf{\\lambda}\\mathbf{V} $$\n",
    "可得：$V$是$M^{T}M$的特征向量，$U$是$MM^{T}$的特征向量, $L$均为两者的特征值，而$\\Sigma$则是特征值开放构成的对角阵\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.11.png width=\"400\" height=\"400\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. 联系PCA和SVD\n",
    "<p align=\"center\">\n",
    "<img src=images/3.3.12.png width=\"400\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Reference:\n",
    "- https://www.bilibili.com/video/BV1E5411E71z?t=1120\n",
    "- 李航，2019，统计学习方法（第二版）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}