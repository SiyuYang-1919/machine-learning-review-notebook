{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Model\n",
    "The model is a classification function:\n",
    "$$ f: R^{n} \\rightarrow {c_{1}, c_{2}, ..., c_{K}} $$\n",
    "\n",
    "### 2. Strategy\n",
    "The probability of misclassification:\n",
    "$$ P(Y != f(X)) = 1 - P(Y=f(X)) $$\n",
    "Assuming training sample $x \\in X$, k nearest samples make up a set $ N_{k}(x) $, if $ N_{k}(x) $ is in the area $ c_{j} $, then the probability of misclassification can be expressed as:\n",
    "$$ \\frac{1}{k} \\sum_{x_{i} \\in N_{k}(x)} I(c_{j} != y_{i}) $$\n",
    "$$ = 1 - \\frac{1}{k} \\sum_{x_{i} \\in N_{k}(x)} I(c_{j} = y_{i}) $$\n",
    "The strategy is to minimize the probability of misclassification, that is, maximize $$\\sum_{x_{i} \\in N_{k}(x)} I(c_{j} = y_{i})$$\n",
    "In this case, $ N_{k}(x) $ should have the largest overlap with the area $ c_{j} $, in other words, using the majority voting rule."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Algorithm\n",
    "![alt text](images/2.2.1.png)\n",
    "![alt text](images/2.2.2.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. $L_{p}$ or Minkowski distance \n",
    "$$ L_{p}(x_{i}, x_{j}) = (\\sum_{l=1}^n |x_{i}^{(l)}-x_{j}^{(l)}|^{p})^{\\frac{1}{p}} $$\n",
    "when $p=2$, we have Euclidean distance,\n",
    "$$ L_{2}(x_{i}, x_{j}) = (\\sum_{l=1}^n |x_{i}^{(l)}-x_{j}^{(l)}|^{2})^{\\frac{1}{2}} $$\n",
    "when $p=1$, we have Manhattan distance,\n",
    "$$ L_{1}(x_{i}, x_{j}) = \\sum_{l=1}^n |x_{i}^{(l)}-x_{j}^{(l)}| $$\n",
    "when $p=\\infty$, we have consider the distance the max distance along each coordinate,\n",
    "$$ L_{\\infty}(x_{i}, x_{j}) = \\max_{l} |x_{i}^{(l)}-x_{j}^{(l)}| $$\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/2.2.3.png\" width=\"200\"/>  <img src=\"images/2.2.4.png\" width=\"200\"/>\n",
    "</center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. the choice of $k$\n",
    "- when $k$ is small, the approximation error will reduce (也就是说只有小范围的训练实例会对预测结果有影响) whereas estimation error will increase (对附近的训练实例很敏感), the model is more complex and more likely to be overfitting;\n",
    "- when $k$ is large, the approximation error will increase (也就是说大范围的训练实例会对预测结果有影响) whereas estimation error will reduce, the model is more simple and more likely to be underfitting;\n",
    "- use cross-validation to pick up the optimal $k$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6. K-D tree\n",
    "![5](images/2.2.5.png)\n",
    "\n",
    "<center class=\"half\">\n",
    "    <img src=\"images/2.2.6.png\" width=\"200\"/>  <img src=\"images/2.2.7.png\" width=\"200\"/>\n",
    "</center>\n",
    "\n",
    "![8](images/2.2.8.png)\n",
    "![8](images/2.2.9.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}