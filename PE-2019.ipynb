{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. (a) Data preparation and understanding\n",
    "#### (1)\n",
    "The main difference between categorical data and ordinal data is that ordinal data have clear ordering, that is, after encoded as numbers, the values of number should reflect their sequence whereas the values of number do not. An example of categirical data: green, blue, red; an example of ordinal data: light green, green, dark green (or low, medium, high).\n",
    "\n",
    "#### (2)\n",
    "1 of K encoding is when there are K categories in a variable, K binary variables will be created. The value in the $i_{th}$ will be 1 for the $i_{th}$ binary variable and 0 otherwise.This method can convert categorical labels into numbers without incorporating the natural ordered relationship in sequential numbers. \n",
    "\n",
    "#### (3)\n",
    "The ordering information will be missed if 1 of K encoding is used for ordinal data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (4)\n",
    "- a. I would choose $r$ to build the predictor as the histogram of this attribute has the least overlap for two classes.\n",
    "- b. I would choose $r, s$ to build the predictor as from the scatterplot of data represents by these two attributes, we can easily see two clusters of data belong to two classes respectively and the decision boundary is very clear as well.\n",
    "- c. I would choose $r, p$ to build the predictor. (?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (b) Clustering\n",
    "#### (1)\n",
    "The K-means algorithm helps to solve the optimal model by performing iterations:\n",
    "- Step 1: initiate centroids and minimize its objective function. The solution to the optimization is to assign each sample to the cluster where its nearest centriod belongs to. Assign each sample.\n",
    "- Step 2: calculate the centroid of each cluster and assign the samples again.\n",
    "- Stop until no clustering assignment changes, that is, the optimization is converged."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (2)\n",
    "The objective function that K-means is optimizing is: \n",
    "$$ \\sum_{j} \\sum_{x_{i} -> C_{j}} ||x_{i} - \\mu_{j}||^2$$\n",
    "This function represents the sum of distances between each sample and the centroid of the cluster the sample is clustered into. This algorithm, a heuristic algorithmi, is very likely to converge to a local minimum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (3)\n",
    "- Calculate the distance between each sample with the starting centroids:\n",
    "$$ d(x_{1}, \\mu_{1}) = 1, d(x_{1}, \\mu_{2}) = 9 $$, assigned to cluster 1\n",
    "$$ d(x_{2}, \\mu_{1}) = 2, d(x_{2}, \\mu_{2}) = 8 $$, assigned to cluster 1\n",
    "$$ d(x_{3}, \\mu_{1}) = 4, d(x_{3}, \\mu_{2}) = 6 $$, assigned to cluster 1\n",
    "$$ d(x_{4}, \\mu_{1}) = 7, d(x_{4}, \\mu_{2}) = 3 $$, assigned to cluster 2\n",
    "$$ d(x_{5}, \\mu_{1}) = 11, d(x_{5}, \\mu_{2}) = 1 $$, assigned to cluster 2\n",
    "$$ d(x_{6}, \\mu_{1}) = 16, d(x_{6}, \\mu_{2}) = 6 $$, assigned to cluster 2\n",
    "$$ d(x_{7}, \\mu_{1}) = 17, d(x_{7}, \\mu_{2}) = 7 $$, assigned to cluster 2\n",
    "- Update the locations of centroids and assign samples again:\n",
    "$$ \\mu_{1} = \\frac{-6-5-3}{3} = \\frac{-14}{3} $$\n",
    "$$ \\mu_{2} = \\frac{0+4+9+10}{4} = \\frac{23}{4} $$\n",
    "$$ d(x_{1}, \\mu_{1}) = \\frac{4}{3}, d(x_{1}, \\mu_{2}) = \\frac{49}{4} $$, assigned to cluster 1\n",
    "$$ d(x_{2}, \\mu_{1}) = \\frac{1}{3}, d(x_{2}, \\mu_{2}) = \\frac{43}{4}  $$, assigned to cluster 1\n",
    "$$ d(x_{3}, \\mu_{1}) = \\frac{5}{3}, d(x_{3}, \\mu_{2}) = \\frac{35}{4} $$, assigned to cluster 1\n",
    "$$ d(x_{4}, \\mu_{1}) = \\frac{14}{3}, d(x_{4}, \\mu_{2}) = \\frac{23}{4} $$, assigned to cluster 1\n",
    "$$ d(x_{5}, \\mu_{1}) = \\frac{26}{3}, d(x_{5}, \\mu_{2}) = \\frac{7}{4} $$, assigned to cluster 2\n",
    "$$ d(x_{6}, \\mu_{1}) = \\frac{41}{3}, d(x_{6}, \\mu_{2}) = \\frac{13}{4} $$, assigned to cluster 2\n",
    "$$ d(x_{7}, \\mu_{1}) = \\frac{44}{3}, d(x_{7}, \\mu_{2}) = \\frac{17}{4} $$, assigned to cluster 2\n",
    "- To know whether the objective function has converged, we only need to test whether the assignment of $x_{5}$ would change:\n",
    "  - The new centroids: $$ \\mu_{1} = \\frac{-6-5-3}{3} = \\frac{-14}{3} $$, $$ \\mu_{2} = \\frac{0+4+9+10}{4} = \\frac{23}{4} $$, have not changed, therefore, the assignment of any sample will not change as well. Two iterations will be enough."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (4)\n",
    "The K-means algorithm can produce a hierarchy of clusters by recursively running the algorithm on points in $C_{i}$; the difference between agglomerative clustering is that HK-means performs a top-down (or devisive) strategy whereas agglomerative clustering uses a bottom-up strategy, which considers each sample a cluster and then emerge these clusters with certain criterions until all the samples are incorporated in a single one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Reference:https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}