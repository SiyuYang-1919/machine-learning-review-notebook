{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Basic structure of a neural network\n",
    "- Simple example only with inputs, a hidden layer, and the output:\n",
    "![1](images/5.1.1.png)\n",
    "\n",
    "- Simple example with inputs, two hidden layers, and the output:\n",
    "![2](images/5.1.2.png)\n",
    "\n",
    "- For multiple classification:\n",
    "![2](images/5.1.3.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Model\n",
    "- For the first hidden layer:\n",
    "$$ a_{k}^{(1)} = g(\\mathbf{w}^T \\mathbf{x_{i}} + b_{0}) $$\n",
    ", where $k = 1,2,3,..,K$ ($K$ is the total number of neurons in the first layer), $i = 1,2,3,...,n$ ($n$ is the total number of features in the original dataset)\n",
    "- For the other hidden layers:\n",
    "$$ a_{k}^{(j)} = g(\\mathbf{w}^T \\mathbf{a_{k}^{(j-1)}} + b_{0}^{(j-1)}) $$\n",
    ", where $j=1,2,...,J $ indicates which layer and $J$ is the total number of hidden layers.  \n",
    "- For the last layer:\n",
    "$$ h(\\mathbf{x}) = g(\\mathbf{w}^T \\mathbf{a_{k}^{(J-1)}} + b_{0}^{(J-1)}) $$ \n",
    "![4](images/5.1.4.png)\n",
    "- Note: the g(x) and h(x) can be a lot of functions:\n",
    "  - (1) Sigmoid function: $ g(x) = \\frac{1}{1 + e^{-x}} $;\n",
    "  - (2) $ g(x) = tanh(x) $\n",
    "  - (3) $ g(x) = x $\n",
    "  - (4) Perceptron: \n",
    "  $$ \n",
    "  g(x) = \n",
    "  \\begin{cases}\n",
    "  1,\\quad x\\geq 0\\\\\n",
    "  -1, \\quad x<0\n",
    "  \\end{cases}\n",
    "  \\tag{1}\n",
    "  $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Strategy - cost function\n",
    "Just like many other machine learning models, the strategy to pick the optimal model is to minimize the cost function.\n",
    "- Regression:\n",
    "$$ C = \\frac{1}{N} \\sum_{i=1}^n (y_{i}-f(\\mathbf{{x}_{i}}))^2 $$\n",
    "- Classification:\n",
    "$$ C = \\sum_{i=1}^n y_{i}logf(\\mathbf{{x}_{i}}) + (1-y_{i})log(1-f(\\mathbf{{x}_{i}})) $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.5.png width=\"500\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.6.png width=\"500\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.7.png width=\"500\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Algorithm - gradient descent and backpropagation\n",
    "- Use gradient descent method:\n",
    "  - Compute the gradient, $ - \\nabla C(\\mathbf{W})$;\n",
    "  - Update the weights by adding the gradient.\n",
    "- Interpret the $ - \\nabla C(\\mathbf{W})$:\n",
    "  - Sign: tells us the direction of each weight should go (increase or decrease);\n",
    "  - Magnitude: indicates what nudges to all of the weights and biases have more importance (cause the fastest change to the cost function) or which changes to which weights matter the most;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Perceptron"
   ]
  }
 ]
}