{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Basic structure of a neural network\n",
    "- Simple example only with inputs, a hidden layer, and the output:\n",
    "![1](images/5.1.1.png)\n",
    "\n",
    "- Simple example with inputs, two hidden layers, and the output:\n",
    "![2](images/5.1.2.png)\n",
    "\n",
    "- For multiple classification:\n",
    "![2](images/5.1.3.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Model\n",
    "- For the first hidden layer:\n",
    "$$ a_{k}^{(1)} = g(\\mathbf{w}^T \\mathbf{x_{i}} + b_{0}) $$\n",
    ", where $k = 1,2,3,..,K$ ($K$ is the total number of neurons in the first layer), $i = 1,2,3,...,n$ ($n$ is the total number of features in the original dataset)\n",
    "- For the other hidden layers:\n",
    "$$ a_{k}^{(j)} = g(\\mathbf{w}^T \\mathbf{a_{k}^{(j-1)}} + b_{0}^{(j-1)}) $$\n",
    ", where $j=1,2,...,J $ indicates which layer and $J$ is the total number of hidden layers.  \n",
    "- For the last layer:\n",
    "$$ h(\\mathbf{x}) = g(\\mathbf{w}^T \\mathbf{a_{k}^{(J-1)}} + b_{0}^{(J-1)}) $$ \n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.4.png width=\"300\" height=\"150\" alt=\"5\" align=centering>\n",
    "- Note: the g(x) and h(x) can be a lot of functions:\n",
    "  - (1) Sigmoid function: $ g(x) = \\frac{1}{1 + e^{-x}} $;\n",
    "  - (2) $ g(x) = tanh(x) $\n",
    "  - (3) $ g(x) = x $\n",
    "  - (4) Perceptron: \n",
    "  $$ \n",
    "  g(x) = \n",
    "  \\begin{cases}\n",
    "  1,\\quad x\\geq 0\\\\\n",
    "  -1, \\quad x<0\n",
    "  \\end{cases}\n",
    "  \\tag{1}\n",
    "  $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Strategy - cost function\n",
    "Just like many other machine learning models, the strategy to pick the optimal model is to minimize the cost function.\n",
    "- Regression:\n",
    "$$ C = \\frac{1}{N} \\sum_{i=1}^n (y_{i}-f(\\mathbf{{x}_{i}}))^2 $$\n",
    "- Classification:\n",
    "$$ C = \\sum_{i=1}^n y_{i}logf(\\mathbf{{x}_{i}}) + (1-y_{i})log(1-f(\\mathbf{{x}_{i}})) $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.5.png width=\"500\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.6.png width=\"500\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.7.png width=\"500\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Algorithm - gradient descent and backpropagation\n",
    "- Use gradient descent method:\n",
    "  - Compute the gradient, $ - \\nabla C(\\mathbf{W})$;\n",
    "  - Update the weights by adding the gradient.\n",
    "- Interpret the $ - \\nabla C(\\mathbf{W})$:\n",
    "  - Sign: tells us the direction of each weight should go (increase or decrease);\n",
    "  - Magnitude: indicates what nudges to all of the weights and biases have more importance (cause the fastest change to the cost function) or which changes to which weights matter the most;\n",
    "\n",
    "- How one training sample affects(trains) the weights?\n",
    "  - Start from the last layer:\n",
    "  <p align=\"center\">\n",
    "<img src=images/5.1.8.png width=\"500\" height=\"300\" alt=\"5\" align=centering>\n",
    "  - We should find ways to make the output closer to the label: change $\\beta$; change $w$; change $a_{k}^{(J-1)}$\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.9.png width=\"500\" height=\"250\" alt=\"5\" align=centering>\n",
    "\n",
    "while we can't change $a_{k}^{(J-1)}$ directly, we should trace back to the previous layers.\n",
    "  - Consider other classes:\n",
    "  <p align=\"center\">\n",
    "<img src=images/5.1.10.png width=\"400\" height=\"250\" alt=\"5\" align=centering>\n",
    "  - Averaging all these changes that we want to make, we expect the following changes to $a_{k}^{(J-1)}$. Repeating the similar process, we change the weights and $a_{k}^{(J-2)}$ to achieve the expected changes in $a_{k}^{(J-1)}$.\n",
    "  <p align=\"center\">\n",
    "<img src=images/5.1.11.png width=\"100\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "- How all the training samples affect(train) the weights?\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.12.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "\n",
    "these averages are proportinal to $ - \\nabla C(\\mathbf{W})$, so that we have\n",
    "$$ - \\nabla C(\\mathbf{W}) = $$\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "-0.08\\\\\n",
    "+0.12\\\\\n",
    "-0.06\\\\\n",
    ":\\\\\n",
    "+0.04\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "- Mini-batch gradient descent (stochastic) (randomly divide samples into small groups and train the neural network with these groups) can make the calculation of gradients much faster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. Detailed calculation behind the algorithm\n",
    "- Think about a simple example first:\n",
    "  - We want to know how changes in the weight impact the cost function:\n",
    "  <p align=\"center\">\n",
    "<img src=images/5.1.13.png width=\"400\" height=\"250\" alt=\"5\" align=centering>\n",
    "  - Actually, we can have - the chain rule:\n",
    "  $$ \\frac{\\partial C_{0}}{\\partial w^{(L)}} = $$\n",
    "  $$ \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_{0}}{\\partial a^{(L)}} $$\n",
    "  - Calculate the derivatives:\n",
    "  $$ \\frac{\\partial C_{0}}{\\partial a^{(L)}} = 2(a^{(L)} - y) $$\n",
    "  $$ \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\sigma^{'} (z^{(L)}) $$\n",
    "  $$ \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = a^{(L-1)}$$\n",
    "  therefore,\n",
    "  $$ \\frac{\\partial C_{0}}{\\partial w^{(L)}} = 2(a^{(L)} - y)\\sigma^{'} (z^{(L)})a^{(L-1)}$$\n",
    "  similarly,\n",
    "  $$ \\frac{\\partial C_{0}}{\\partial b^{(L)}} = 2(a^{(L)} - y) \\sigma^{'} (z^{(L)})$$\n",
    "  $$ \\frac{\\partial C_{0}}{\\partial a^{(L-1)}} = 2(a^{(L)} - y) \\sigma^{'} (z^{(L)})w^{(L)}$$\n",
    "  - For all the training examples, we have:\n",
    "  $$ \\frac{\\partial C}{\\partial w^{(L)}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\frac{\\partial C_{k}}{\\partial w^{(L)}} $$\n",
    "  - Overall, the graident should be:\n",
    "  $$ \\nabla C(\\mathbf{W}) = $$\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial C}{\\partial w^{(1)}}\\\\\n",
    "\\frac{\\partial C}{\\partial b^{(1)}}\\\\\n",
    ":\\\\\n",
    "\\frac{\\partial C}{\\partial w^{(L)}}\\\\\n",
    "\\frac{\\partial C}{\\partial b^{(L)}}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.14.png width=\"400\" height=\"200\" alt=\"5\" align=centering>\n",
    "<p align=\"center\">\n",
    "<img src=images/5.1.15.png width=\"400\" height=\"200\" alt=\"5\" align=centering>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Perceptron"
   ]
  }
 ]
}