{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1. Assumption of Naive Bayes\n",
    "Naive Bayes assumes that the probability distribution of the values in each attribute is independent given the class, therefore, we have\n",
    "$$ P(X=x|Y=c_{k}) = P(X^{(1)}=x^{(1)},..., X^{(n)}=x^{(n)}|Y=c_{k}) = \\prod_{j=1}^n \\quad P(X^{(i)}=x^{(i)}|Y=c_{k}) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Naive Bayes model (general)\n",
    "$$ y = \\arg \\max_{c_{k}} P(Y=c_{k}) \\prod_{j} P(X^{(j)}=x^{(j)}|Y = c_{k}) $$\n",
    "or \n",
    "$$ y = \\arg \\max_{c_{k}} \\frac{P(Y=c_{k}) \\prod_{j} P(X^{(j)}=x^{(j)}|Y = c_{k})}{\\sum_{k}^K P(Y=c_{k}) \\prod_{j} P(X^{(j)}=x^{(j)}|Y = c_{k})}$$\n",
    "\n",
    "Remember there are at least two parameters in Naive Bayes models: one is $P(Y=c_{k})$ and another is $P(X^{(j)}=x^{(j)}|Y = c_{k})$\n",
    "- For a Bernouli model:\n",
    "$$ P(Y=c_{k}) = \\frac{\\sum_{i=1}^N I(y_{i} = c_{k})}{N} $$\n",
    "$$ P(X^{(j)}=x^{(j)}|Y = c_{k}) = \\frac{\\sum_{i=1}^N I(x_{i}^{(j)}=a_{jl}, y_{i} = c_{k})}{\\sum_{i=1}^N I(y_{i} = c_{k})} $$\n",
    "$$ P(Y=c_{k}) = \\frac{\\sum_{i=1}^N I(y_{i} = c_{k})}{N} $$\n",
    "$$ P(X^{(j)}=x^{(j)}|Y = c_{k}) = \\frac{\\sum_{i=1}^N I(x_{i}^{(j)}=a_{jl}, y_{i} = c_{k})}{\\sum_{i=1}^N I(y_{i} = c_{k})} $$\n",
    "$$ \\pi_c = \\frac {N_c} {N_D} $$\n",
    "$$ \\theta_jc = \\frac {N_jc} {N_c}$$\n",
    "$$ \\theta_jc = \\frac {\\alpha + N_jc} {2\\alpha + N_c}$$\n",
    "- For a multinomial model:\n",
    "$$ P(Y=c_{k}) = \\frac{\\sum_{i=1}^N I(y_{i} = c_{k})}{N} $$\n",
    "$$ P(X^{(j)}=x^{(j)}|Y = c_{k}) = \\frac{\\sum_{i=1}^N I(w_{t}=1, y_{i} = c_{k})x_{i}^{(t)}} {\\sum_{i=1}^N \\sum_{s=1}^d I(w_{s}=1, y_{i} = c_{k})x_{i}^{(s)}} $$\n",
    "$$\\pi_j = \\frac {N_j}{N}$$ \n",
    "$$p(w_i|c) = \\theta_{ij} = \\frac {count(w_i, c_j)}{\\sum_{w \\in V} count(w, c_j)}$$\n",
    "$$ p(w_i|c) = \\frac {count(w_i, c_j)+1}{\\sum_{w \\in V} count(w, c_j)+1}$$\n",
    "$$ = \\frac {count(w_i, c_j)+1}{(\\sum_{w \\in V} count(w, c_j)) + |V|}$$\n",
    "- For Gaussian model:\n",
    "$$ P(Y=c_{k}) = \\frac{\\sum_{i=1}^N I(y_{i} = c_{k})}{N} $$\n",
    "$$ p(x=v|C_k) = \\frac{1}{\\sqrt{{2\\pi}{\\sigma_k}^2}} e^{-\\frac {(v-{\\mu_k}^2)}{2{\\sigma_k}^2}} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Different Naive Bayes models\n",
    "According to different assumptions on the distribution of the attributes, we can get different Naive Bayes models:\n",
    "- The probability distribution of the given data can be quite different (the following three types of Naive Bayes assume different probability distributions), but the basic idea is to estimate the parameters for a feature's distribution. One must assume a distribution or generate nonparametric models for the features from the training set.\n",
    "- The assumptions on distributions of features are called the \"event model\" of the naïve Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), ```multinomial``` and ```Bernoulli``` distributions are popular. These assumptions lead to two distinct models, which are often confused.\n",
    "- Although we assume the models (distributions of features), we still need paramtertes in the models to use them. While it is almost unlikely to get the probability density function of these features (because of the samples or maybe the features are too many), we can simply use the training data to train the model and get the parameters that, under the assumed statistical model, make the observed data most probable. This is where the MLE is applied. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.1 Gaussian Naive Bayes:\n",
    "  - Typical assumption: the continuous values associated with each class are distributed according to a normal (Gaussian) distribution. For example, suppose we have collected some observation value $v$. Then, the probability distribution of $v$ given a class $C_{k}$, $p(x=v\\mid C_{k})$, can be computed by plugging $v$ into the equation for a normal distribution parameterized by $\\mu _{k}$ and $\\sigma _{k}^{2}$. That is,\n",
    "$$ p(x=v|C_k) = \\frac{1}{\\sqrt{{2\\pi}{\\sigma_k}^2}} e^{-\\frac {(v-{\\mu_k}^2)}{2{\\sigma_k}^2}} $$\n",
    "  ![alt text](https://miro.medium.com/max/1400/1*YeSlHB90J4rviMiG0Su6tA.png)\n",
    "    - One example is the one that classfies children and adults through their height, weight, and foot size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.2 Bernouli Naive Bayes:\n",
    "- Bernouli disribution can be expressed as $$ P(X=x) = p^x(1-p)^{(1-x)} $$\n",
    ", where $x=1$, $P(X=1)=p$; $x=0$, $P(x=0)=1-p$\n",
    "- When applied to Naive Bayes, we can get \n",
    "$$ p(X|C_k) = \\prod_{i=1}^n {{p_k}_i}^{x_i} (1-{{p_k}_i})^{(1-x_i)} $$\n",
    "- That is,\n",
    "  $$ p(C_k|X) ∝ p(C_x) \\prod_{i=1}^n {{p_k}_i}^{x_i} (1-{{p_k}_i})^{(1-x_i)} $$\n",
    "  When expresses in log-space, the MNB becomes a linear classifier:\n",
    "  $$ log (p(C_k|X)) ∝ log (p(C_x) \\prod_{i=1}^n {{p_k}_i}^{x_i} (1-{{p_k}_i})^{(1-x_i)}$$\n",
    "  $$ = log (p(C_k)) + {x_i} \\sum_{i=1}^n log {{p_k}_i} + (1-x_i)\\sum_{i=1}^n log (1-{p_k}_i) $$\n",
    ", where two parameters in this model are $p_c$ or $\\pi$ in $b = log (p(C_k))$ and ${p_k}_i$ or $\\theta$ in ${{w_k}_i} = log ({p_k}_i)$, respectively.\n",
    "- Now, we can rewrite the formula above and convert it to a likelihood function:\n",
    "  $$ L(\\pi, \\theta) = p(y^{(i)}|\\pi) \\prod_{j=1}^n p(x_{j}^{(i)}|\\theta_{j})$$\n",
    "  $$ lnL(\\pi, \\theta) = \\sum_{i} {(lnp(y^{(i)}|\\pi)} + \\sum_{j} {(lnp(x_{j}^{(i)}|\\theta_{j})} $$\n",
    "solving two parameters through partial derivatives, we get:\n",
    "$$ \\pi_c = \\frac {N_c} {N_D} $$\n",
    "$$ \\theta_jc = \\frac {N_jc} {N_c}$$\n",
    "- Just as the multinomial Naive Bayes, we need to deal with the situation when zero probability occurs. To avoid this problem, we have:\n",
    "$$ \\theta_jc = \\frac {\\alpha + N_jc} {2\\alpha + N_c}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.3 Multinomial Naive Bayes:\n",
    "  - The general model is (the same as other Naive Bayes classifiers):\n",
    "  $$ p(C_k | x_1, ..., x_n) $$\n",
    "  - Also, in Multinomial Naive Bayes, we have:\n",
    "  $$ p(X|C_k) = \\frac {n!}{\\prod_{i=1}^n x_i!} \\prod_{i} {{p_k}_i}^{x_i} $$\n",
    "  - That is (omitting $\\frac {n!}{\\prod_{i=1}^n x_i!}$),\n",
    "  $$ p(C_k|X) ∝ p(C_x) \\prod_{i=1}^n {{p_k}_i^{x_i}} $$\n",
    "  When expresses in log-space, the MNB becomes a linear classifier:\n",
    "  $$ log (p(X|C_k)) ∝ log (p(C_x) \\prod_{i=1}^n {{p_k}_i}^{x_i})$$\n",
    "  $$ = log (p(C_k)) + \\sum_{i=1}^n {x_i} \\textrm{*} log ({p_k}_i) $$\n",
    "  $$ = b + w_{k}^T x $$\n",
    "  , where two parameters in this model are $p_c$ or $\\pi_j$ in $b = log (p(C_k))$ and ${p_k}_i$ or $\\theta_ij$ in ${{w_k}_i} = log ({p_k}_i)$, respectively.\n",
    "  $$ L(\\pi, \\theta) = p(C_x) \\prod_{i=1}^n {{p_k}_i^{x_i}} $$\n",
    "  $$ log L(\\pi, \\theta) = log (p(C_x) \\prod_{i=1}^n {{p_k}_i}^{x_i})$$\n",
    "  $$ = log (p(C_k)) + \\sum_{i=1}^n {x_i} \\textrm{*} log ({p_k}_i) $$\n",
    "  - We use the training data to find two parameters that make the observed data most probable under this multinomial model, and we get:\n",
    "$$\\pi_j = \\frac {N_j}{N}$$ \n",
    "$$p(w_i|c) = \\theta_{ij} = \\frac {count(w_i, c_j)}{\\sum_{w \\in V} count(w, c_j)}$$. However, there is a problem as maybe we have seen no training data tell us a word has been classified in a specific class. In this case, zero probability cannot be conditioned away. That's why we have Laplace (add-1) smoothing:\n",
    "$$ p(w_i|c) = \\frac {count(w_i, c_j)+1}{\\sum_{w \\in V} count(w, c_j)+1}$$\n",
    "$$ = \\frac {count(w_i, c_j)+1}{(\\sum_{w \\in V} count(w, c_j)) + |V|}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Deal with missing data\n",
    "- Ignore attribute in insatnce where its value is missing;\n",
    "- No need to 'fill in' any values;\n",
    "![1](images/2.3.1.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}